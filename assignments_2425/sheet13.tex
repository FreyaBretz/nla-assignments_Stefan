%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Numerical Linear Algebra class 2024
% Sheet 13
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{Sheet}[Problems for presentation on Thursday, 30 January]
  \label{sheet13}
  
  \begin{Problem}
      Problem 1.2.9 in the lecture notes.
  \end{Problem}

    \begin{Problem}
        Problem 2.1.24 in the lecture notes.
    \end{Problem}

    \begin{Problem}
        Let $\mata$ be a symmetric tridiagonal matrix. Show that the
        QR-iteration (see Algorithm 2.4.2 in the lecture notes) preserves
        the tridiagonal structure of the matrix, i.e., all iterates
        $\mata^{(n)}$ generated by the QR-iteration are tridiagonal.
     \end{Problem}

    \begin{Problem}
        Problem 2.4.18 in the lecture notes.
    \end{Problem}

    \begin{Problem}
        Let $\matH$ be a real Hessenberg matrix.
        Let $\matq_1,\matr_1,\matq_2,\matr_2$ be the matrices obtained in the explicit double-shift QR step (Algorithm  2.5.14).
        Let $\sigma_1$ and $\sigma_2$ are the shifts in Lemma 2.5.16.
        Show that the following equation in the proof of Lemma 2.5.16 holds:
        \begin{equation*}
            \matq_1\matq_2\matr_2\matr_1 = \matm = (\matH - \sigma_1\id)(\matH - \sigma_2\id)
        \end{equation*}
    \end{Problem}

  \begin{Problem}
    Prove Theorem 3.2.15 in the lecture notes.
  \end{Problem}

	\begin{Problem}
        Consider the linear system $\mata\vx = \vb$, where $\mata$ is a
        symmetric positive definite matrix. We define a projection method
        which uses a two-dimensional space at each step. At a given step,
        take $L = K = \spann{\vr, \mata\vr}$, where $\vr = \vb - \mata\vx$
        is the current residual.
        \begin{enumerate}[(a)]
            \item For a basis of $K$ use the vector $\vr$ and the vector $\vp$
            obtained by orthogonalizing $\mata\vr$ against $\vr$ with
            respect to the $\mata$-inner product. Give the formula for
            computing $\vp$ (no need to normalize the resulting vector).
            \item Write the algorithm for performing the projection method
            described above.
            \item Can you explain, not compute, why the algorithm converges for any initial guess $\vx_0$?
            \textit{Hint: Exploit the convergence results for one-dimensional projection techniques.}
        \end{enumerate}
	\end{Problem}

    \begin{Problem}
        Consider a matrix of the form
         \[\mata = \identity + \alpha\matb, \]
         where $\matb$ is skew symmetric (real), i.e., such that $\matb^T = -\matb$.
         \begin{enumerate}[(a)]
             \item Show that $\langle\mata\vx,\vx\rangle/\langle\vx,\vx\rangle = 1$ for all $\vx \neq 0$.
             \item Consider the Arnoldi process for $\mata$.
                 Show that the resulting Hessenberg matrix will have the following tridiagonal form:
                 \[ \matH_m = \begin{pmatrix}
                     1 & -\eta_2 & & & \\
                     \eta_2 & 1 & -\eta_3 & & \\
                       &  & \cdots & & \\
                       &  & \eta_{m-1} & 1 & -\eta_m \\
                       &  &  & \eta_m & 1 
                 \end{pmatrix}.\]
             \item Using the result of part (b), explain why the CG algorithm applied as is to a linear system with  the matrix $\mata$, which is nonsymmetric,
                 will still yield residual vectors that are orthogonal to each other.
         \end{enumerate}
    \end{Problem}


  \vfill
  \bibliographystyle{alpha}
  \bibliography{bib}

\end{Sheet}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End:
