%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Numerical Linear Algebra class 2022 
% Sheet 1                             
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{Sheet}[to be handed in until October 28, 2022, 2pm.]
  
\begin{Problem}
  Review the following items and write down at least one of the
  definitions and one of the theorems with proof in detail.
  \begin{itemize}
  \item Definition of a projection
  \item Definition of an orthogonal projection
  \item Theorem: Orthogonal projection is uniquely determined by
    subspace

    Consider a finite-dimensional space $V$ with inner product
    $\scal(\cdot,\cdot)$ and a subspace $W\subset V$. Then, there
    exists a unique orthogonal projection
    \begin{gather*}
       P_W:V\to W.
    \end{gather*}
  \item Theorem: Best Approximation Theorem

    Let $W$ be a subspace of $\R^n$, let $\bm x$ be any vector in
    $\R^n$, and let $\tilde{\bm x}$ be the orthogonal projection of
    $\bm x$ onto $W$. Then $\tilde{\bm x}$ is the closest point in $W$
    to $\bm x$, in the sense that
    \begin{gather*}
      \norm{\bm x - \tilde{\bm x}} < \norm{\bm x - \bm w}
    \end{gather*}
    for all $\bm w$ in $W$ distinct from $\bm x$.
  \item Theorem: Orthogonal projection in orthonormal basis
    
    Let $B = \{\bm u_1,\bm u_2, ...,\bm u_p\}$ be an orthonormal basis
    of a subspace $W$ of a finite-dimensional space $V$ with inner
    product $\scal(\cdot,\cdot)$. Then, the orthogonal projection
    $P_i$ of any vector $\bm v\in V$ onto $\bm u_i$, and the
    orthogonal projection $P_W$ of any vector $\bm v\in V$ onto $W$
    have the following expressions, respectively:
    \begin{gather*}
      P_i(\bm v) = \scal(\bm v, \bm u_i) \bm u_i,\qquad i = 1, 2, ..., p,
      \\
      P_W(\bm v) = \sum_{i=1}^p \scal(\bm v, \bm u_i) \bm u_i,
    \end{gather*}
    and
    \begin{gather*}
      \bm v = P_W(\bm v) + \bm z,\qquad \bm z \perp W.
    \end{gather*}

  \item Theorem: Parseval identity

    Suppose that $W$ is a finite-dimensional linear space with inner
    product $\scal(\cdot,\cdot)$. Let $\{\bm e_i\}$, $i=1,...,n$ be an
    orthonormal basis of $W$. Then, for every $\bm w\in W$ it holds
    \begin{gather*}
      \sum_{i=1}^n \abs{\scal(\bm w,\bm e_i)}^2
      = \sum_{i=1}^n \norm{\bm w}^2.
    \end{gather*}
  \end{itemize}
\end{Problem}

\begin{Problem}
  Proof that every finite-dimensional vector space with scalar product
  has an orthonormal basis.

  \textit{Hint:} Gram-Schmidt orthogonalization
\end{Problem}

\begin{Problem}
  Compute the eigenvalues and eigenvectors of the matrix
  \begin{gather*}
    \begin{bmatrix}
      \cos(\alpha) & \sin(\alpha) \\
      -\sin(\alpha) & \cos(\alpha)
    \end{bmatrix},
  \end{gather*}
  for $\alpha\in\R$.
\end{Problem}

\begin{Problem}[Programming]
  Write a program that:
  \begin{itemize}
  \item Generates an orthonormal basis from a given set of $n$
    $n$-dimensional complex vectors.
  \item Computes the Gram matrix of the obtained orthonormal basis
  \end{itemize}
  Test your program on the set of vectors: $\{\bm v_k\}$, $k=1,...,n$,
  $(\bm v_k)_j = e^{\frac{i\alpha_k j}{n}}$, $\alpha_k = 1 +
  \frac1k$. Investigate the obtained Gram matrix.
\end{Problem}

\end{Sheet}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End: 
